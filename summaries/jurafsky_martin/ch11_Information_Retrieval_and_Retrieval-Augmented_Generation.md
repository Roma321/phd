В данной главе проводится обзор задач информационного поиска и **QA (question answering) систем** в частности.

Для QA-систем авторы приводят 2 типа вопросов:
1. Factoid questions — фактоидные вопросы — категория вопросов, на которые можно дать простой ответ, вопросы о фактах (Где находится Лувр).
2. Reasoning questions — вопросы на размышление — требующие логического размышления.

Затем приводится список слабостей LLM в этой задаче
1. **Галлюцинации.**
2. Отсутствие **"калибровки"**. **Откалиброванной** называют систему, уверенность которой имеет высокий коэффициент корреляции с вероятностью того, что ответ, данный ей, корректен.
3. Отсутствие доступа к **частным данным** (которых не было в обучающем датасете, например, писем юзера).
4. Знания LLM **статичны** — она не может получить информацию, которая ей не была предоставлена в момент обучения.

В качестве способа решения описанных проблем чаще всего используется **генерация, дополненная поиском (retrieval-augmented generation, RAG)**. Выполняется стандартный информационный поиск по пользовательскому запросу, а затем найденные документы подаются на вход LLM вместе с пользовательским запросом и промптом по типу "На основе данных документов ответь на вопрос". Авторы называют такую схему **Retriever-reader**.

### Информационный поиск
**Информационный поиск (Information retrieval, IR)** — задача нахождения всех документов или медиа, соответствующих пользовательскому запросу. Полученная система называется **поисковым движком (search engine)**. 

Рассматривается задача **поиска по запросу (ad hoc retrieval)**, возвращающая упорядоченный набор **документов** из некоторой большей **коллекции**.

**Документ** — любой элемент, индексируемый системой

**Коллекция** — набор документов, который использовался для удовлетворения запроса

**Термин** — слово, n-грамма или фраза из коллекции 

**Запрос** — информационная потребность пользователя, выраженная набором терминов.

В основном, IR-системы используют векторное представление запросов и документов.


#### Информационный поиск с разреженными векторами.
Это классический подход, в котором лишь несколько координат вектора поискового запроса, соответствующие использованным терминам, не являются нулевыми. Модель *мешка слов* не используется, но используется **tf-idf (term frequency-inverce document frequency)**, придающая разную занчимость разным терминам из запроса.

Идея такая, что **tf** показывает насколько часто термин встречается в документе — чем чаще, тем выше вероятность, что документ посвящён этому термину. **Idf** показывает, насколько термин редко встречается в принципе. Чем реже, тем выше специфичность термина, тем он более важен в пользовательском запросе.

**Tf** термина **t** по документу **d** считается следующим образом:

$$
\mathrm{tf}_{t,d} =
\begin{cases} 
1 + \log_{10} \mathrm{count}(t,d), & \text{если } \mathrm{count}(t,d) > 0 \\[2mm]
0, & \text{иначе}
\end{cases}
$$

**Idf** постоянна и не зависит от конкретного документа. **N** — общее число документов, $\mathrm{df}_t$ — в скольких документах встретился термин **t**

$$
\mathrm{idf}_{t} = \log_{10}\frac{N}{\mathrm{df}_t}
$$

Сама метрика **tf-idf** для термина **t** в документе **d** считается по формуле

$$
\mathrm{tf}\text{-}\mathrm{idf} = \mathrm{tf}_{t,d} \cdot \mathrm{idf}_{t}
$$

После того, как мы представили в виде **tf-idf** векторов запрос и документы, мы ищем метрику схожести запроса и документа как косинус угла между векторами документа и запроса.

$$
\text{score}(q, d) = \sum_{t \in q} \frac{\text{tf}_{t,q} \cdot \text{idf}_t}{\sqrt{\sum_{q_i \in q} \text{tf-idf}^2(q_i, q)}} \cdot \frac{\text{tf}_{t,d} \cdot \text{idf}_t}{\sqrt{\sum_{d_i \in d} \text{tf-idf}^2(d_i, d)}}
$$

Есть варианты упрощения формулы, например, можно избавиться от одного из множителей $\mathrm{idf}_t$. Другие упрощения избавдяются от других частей формулы.

Существует другая формула из "семейства" **tf-idf** — **BM25 (Okapi BM25, Okapi)**. Идея заключена в добавлении 2 дополнительных параметров в формулу **tf-idf**. Параметр **k** отвечает за соотношение важностей **tf** и **idf**, а параметр **b** "штрафует" длинные документы, делая менее важным количество вхождений термина в них.

$$
\text{score}(q, d) = \sum_{t \in q} \text{idf}_t \cdot \frac{\text{tf}_{t,d} \cdot (k_1 + 1)}{\text{tf}_{t,d} + k_1 \cdot \left(1 - b + b \cdot \frac{|d|}{|d_{\text{avg}}|}\right)}
$$

- $ |d| $ — длина документа $ d $ (в словах).
- $ |d_{\text{avg}}| $ — средняя длина документа в коллекции.

При $k = 0$ частота вхождений становится бинарной — термин либо входит либо нет. При $k \to \infty$ "насыщения" частоты не происходит и она остаётся неизменной по сравнению с исходной формулой. 

При $b = 0$ штрафов за длину документа нет. При $b = 1$ штрафы становятся довольно сильными.

Опытным путём выяснено, что наиулучшие результаты получаются при $b = 0.75$ и $k=[1.2,2]$, хотя есть и другие вариации.

* **Стоп слова** — термины, которые встречаются так часто, что практически не несут смысловой нагрузки. Их можно удалить, что может несколько ускорить поиск. В современных IR-системах это подход не используется, посколько может "порезать" запросы по типу *to be or not to be*, а также невозможным поиск по точной фразе. В то же время, в системах, для которых важна только семантика, этот подход всё ещё может быть применён.

#### Инвертированный индекс
Это способ хранения информации о коллекции, который используется для задачи информационного поиска. В базовом виде представляет собой словарь "Термин-список вхождений". Может содержать дополнительную информацию: частоту вхождений термина (idf), позиции термина в документе, а не только факт его вхождения.

#### Техники оценки систем информационного поиска

Можно использовать классические метрики: точность, полноту, $F$-меру. 
Пусть для некоторого запроса 
* $R$ — число релевантных документов, выданных системой
* $T$ — число документов, выданных системой
* $U$ — общее число релевантных документов в системе

Тогда
$$
Precision = \frac{|R|}{|T|}\text{      }Recall = \frac{|R|}{|U|}
$$

Для оценивания можно построить график "точность-полнота" (полнота по оси X). Для каждого i-го из T документов (начиная с 1го берётся значение точности и полноты, посчитанные для первых i документов), отмечается и строится график. Стоит отметить, что полнота не убывает, поэтому её естественно использовать как ось X и строить зависимость точности от полноты.

Чтобы точность не "скакала" с каждым новым i-м корректным документом в коллекции вместо неё используют **интерполированную точность**, выражаемую как
$$
\mathrm{IntPrecision}(r) = \max_{i\geq r} \mathrm{Precision}(i)
$$

(берём максимальное значение точности, доступное при заданном уровне полноты)

Следующий способ оценки — средняя точность **(MAP)**. Эта метрика учитывает ранжирование поисковой системы. Считается так: запускается цикл сверху выдачи. Если очередной i-й документ является релевантным, то считается точность до i-го элемента. Посчитанные точности складываются и усредняются.

Для 1 запроса:

$$
\text{AP} = \frac{1}{|R_r|} \sum_{d \in R_r} \text{Precision}_r(d)
$$
Следовательно, для Q запросов:
$$
\text{MAP} = \frac{1}{|Q|} \sum_{q \in Q} \text{AP}(q)
$$.

#### Информационный поиск с плотными векторами

Вместо классических векторов, похожих на **tf-idf**, в которых большая часть координат равна нулю можно попробовать более плотные вектора — эмебддинги, получаемые, например, как выход перед линейным слоем BERT. Такой подход побеждает такую проблему как **vocabulary mismatch problem**, разрешая синонимию.

Наиболее точный результат даст подход, при котором запрос поочерёдно с каждыи документом подаётся на вход BERT, разделённые специальным токеном [SEP]

$$
\mathbf{z} = \text{BERT}(q; [\text{SEP}]; d)[\text{CLS}]
$$

$$
\text{score}(q, d) = \text{softmax}(\mathbf{U} \mathbf{z})
$$

Как бы этот подход не был эффективен, он бесполезен, поскольку требует прогонять BERT каждой паре запрос-документ, а это много ресурсов.

В качестве альтернативы предлагется подход **bi-encoder** (двойной энкодер)
От докумета и от запроса считается BERT-вектор, затем используется косинусная мера сходства.

Есть промежуточные решения — с помощью более простого мметода, как **BM25** отобрать набор документов, а затем воспользоваться подходом №1 (с загрузкой в BERT каждой пары документ-запрос).

Ещё одно промежуточное, решение, заслуживающее внимания, это  **ColBERT**.
Он похож на **bi-encoder**, но вместо векторизации документов и запросов используется векторизация каждого термина (с учётом контекста). Затем мера схожести определяется как сумма наиболее подходящих терминов документа к каждому термину из запроса.

$$
\mathrm{score}(q,d) = \sum_{i = 1}^{N}\max_{j=1}^m E_{q_i}\cdot E_{d_j}
$$

Методы, использующие плотные вектора, не могут использовать инвертированный индекс для отбора наиболее подходящих документов, поскольку эмебддинги не хранят в чистом виде информацию об использованных терминах. Вместо этого используются специализированные алгоритмы, такие как **nearest neighbours search** или **FAISS**

### QA-системы, дополненные поиском

Как было сказано выше, современных подход к построению QA-систем — это **retrieval-augmented generation (RAG, генерация, дополненная поиском)**. Система состоит из 2 частей **Retriever** и **Reader**.

Некоторые вопросы требуют нескольких проходов от ретривера к ридеру:
по запросу выполняется поиск документов, на основе множества документов дополняется вопрос и так повторяется пока не будет сформулирован достаточно точный ответ пользователю. Задача о более плотной интеграции ридера с ретривером явялется одной из актуальных исследовательских задач и сейчас. 

Введём понятие **текстовый пассаж** — это фрагмент текста, воспринимаемый как отдельная смысловая единица. 

#### Датасеты для задач QA
* **Natural Questions** — английские фактоидные запросы в гугл с кратким и полным ответом
* **MS MARCO** (Microsoft Machine Reading Comprehension) — 1 млн запросов с 9 млн ответов (разной степени точности) на английском 
* **TyDIQA** — мультиязычный корпус, в котором запросу + набору пассажей соответсвуют номера пассажей, содержащих ответ на запрос.
* **MMLU** — корпус заданий с экзаменов, уровня примерно магистратуры в разных областях

Можно разбить корпуса (и задачи) на задачи QA **open book**, когда системе датся набор пассажей, серди которых есть корректный ответ и **closed book**, когда дополнительной информации не даётся.

#### Оценка QA-систем
Для задач выбора варианта (единственного или множественного) используется метрика точного совпадения — процент ответов модели, совпавших с эталоном.

Для задач с ответом с свободной форме используется $F_1$-мера, рассчитанная через перекрытие слов в эталоне и в ответе.

Для оценки ранжирования можно использовать **среднеобратный ранг (mean reciprocal rank, MRR)**. 

$$
\text{MRR} = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\text{rank}_i}.
$$

### Итог

В этой главе представлены задачи **ответа на вопросы** и **поиска информации**.

- **Ответ на вопросы (QA)** — это задача ответа на вопросы пользователя.
- В этой главе мы сосредоточимся на задаче поисково-ориентированного ответа на вопросы, в которой вопросы пользователя предполагается отвечать с помощью материала из некоторого набора документов (которым может быть веб).
- **Поиск информации (IR)** — это задача возврата документов пользователю на основе его информационной потребности, выраженной в виде запроса. В ранжированном поиске документы возвращаются в порядке их релевантности.
- Сопоставление между запросом и документом можно выполнить, сначала представив каждый из них разреженным вектором, который отражает частоты слов, взвешенные по `tf-idf` или `BM25`. Затем схожесть можно измерить с помощью косинусной меры.
- Документы или запросы можно вместо этого представлять плотными векторами, кодируя вопрос и документ с помощью модели только для кодирования, такой как BERT, и в этом случае вычисляя схожесть в пространстве вложений.
- **Инвертированный индекс** — это механизм хранения, который позволяет очень эффективно находить документы, содержащие определённое слово.
- Ранжированный поиск обычно оценивается по **средней точности** или **интерполированной точности**.
- Системы ответа на вопросы обычно используют архитектуру **ретривер/ридер**. На этапе **извлечения** система поиска информации получает запрос и возвращает набор документов.
- Этап **чтения** реализуется с помощью **генерации, дополненной поиском**, в которой крупная языковая модель получает запрос и набор документов, а затем условно генерирует новый ответ.
- QA можно оценивать по **точному совпадению** с известным ответом, если дан только один ответ, по **F₁-мере** для ответов в свободной форме или по **среднему обратному рангу**, если дан ранжированный набор ответов.

### Мои выводы

* LLM пока не покрывают все задачи в теме информационного поиска и Quesion-answering
* Большинство современных QA-систем используют **RAG** — генерацию, дополненную поиском
* Актуальным направлением исследований являются попытки включить ридер, LLM в первый этап получения информации, то есть более плотную интеграцию LLM с задачей информационного поиска